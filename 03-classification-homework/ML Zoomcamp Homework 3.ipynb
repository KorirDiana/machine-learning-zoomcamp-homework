{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a10435-7005-4832-b79a-5fbdaf79af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lead_source    industry  number_of_courses_viewed  annual_income  \\\n",
      "0      paid_ads         NaN                         1        79450.0   \n",
      "1  social_media      retail                         1        46992.0   \n",
      "2        events  healthcare                         5        78796.0   \n",
      "3      paid_ads      retail                         2        83843.0   \n",
      "4      referral   education                         3        85012.0   \n",
      "\n",
      "  employment_status       location  interaction_count  lead_score  converted  \n",
      "0        unemployed  south_america                  4        0.94          1  \n",
      "1          employed  south_america                  1        0.80          0  \n",
      "2        unemployed      australia                  3        0.69          1  \n",
      "3               NaN      australia                  1        0.87          0  \n",
      "4     self_employed         europe                  3        0.62          1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84ef1eb8-95ea-4abc-8885-a39a243c8c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Show number of missing values per column\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d5574-b86b-404a-864f-94d0d83b2e2f",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "**What is the most frequent observation (mode) for the column industry?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f867a191-0aa6-4ed8-85cd-861f1902fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent industry: retail\n"
     ]
    }
   ],
   "source": [
    "# Find the mode (most frequent value) in the 'industry' column\n",
    "mode_industry = df['industry'].mode()[0]\n",
    "\n",
    "print(\"Most frequent industry:\", mode_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf9e03-bae0-457d-954b-95bcf26c5e54",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0733c2f9-924f-47c8-90f1-099f898e8501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction_count and lead_score: 0.009888182496913131\n",
      "number_of_courses_viewed and lead_score: -0.004878998354681276\n",
      "number_of_courses_viewed and interaction_count: -0.023565222882888037\n",
      "annual_income and interaction_count: 0.02703647240481443\n"
     ]
    }
   ],
   "source": [
    "# Clean missing values\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].fillna('NA')\n",
    "df[numerical_cols] = df[numerical_cols].fillna(0.0)\n",
    "\n",
    "# Create correlation matrix for numerical features\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Print correlation values for the specified pairs\n",
    "print(\"interaction_count and lead_score:\", corr_matrix.loc['interaction_count', 'lead_score'])\n",
    "print(\"number_of_courses_viewed and lead_score:\", corr_matrix.loc['number_of_courses_viewed', 'lead_score'])\n",
    "print(\"number_of_courses_viewed and interaction_count:\", corr_matrix.loc['number_of_courses_viewed', 'interaction_count'])\n",
    "print(\"annual_income and interaction_count:\", corr_matrix.loc['annual_income', 'interaction_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e616de09-a0bd-4733-9bc7-a0b0694afcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 877\n",
      "Validation size: 292\n",
      "Test size: 293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate target\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "# First split: train (60%), temp (40%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Second split: val (20%), test (20%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Check sizes\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb875f-cc47-4baa-a549-53df8a5dc97e",
   "metadata": {},
   "source": [
    "**biggest correlation:** annual_income and interaction_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724235a9-3ea5-43d4-8db7-0b55452a2be2",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Calculate the mutual information score between converted and other categorical variables in the dataset. Use the training set only.\n",
    "Round the scores to 2 decimals using round(score, 2).\n",
    "Which of these variables has the biggest mutual information score?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "094eeb49-7d6c-46be-bdc0-865fd74e1616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores:\n",
      "industry: 0.02\n",
      "location: 0.0\n",
      "lead_source: 0.03\n",
      "employment_status: 0.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Only using training data\n",
    "categorical = ['industry', 'location', 'lead_source', 'employment_status']\n",
    "\n",
    "# Convert categorical variables to category dtype\n",
    "for col in categorical:\n",
    "    X_train[col] = X_train[col].astype('category')\n",
    "\n",
    "# Encode categories with category codes\n",
    "X_train_encoded = X_train.copy()\n",
    "for col in categorical:\n",
    "    X_train_encoded[col] = X_train[col].cat.codes\n",
    "\n",
    "# Compute mutual information\n",
    "mi_scores = mutual_info_classif(X_train_encoded[categorical], y_train, discrete_features=True)\n",
    "\n",
    "# Round scores to 2 decimals\n",
    "mi_scores_rounded = {col: round(score, 2) for col, score in zip(categorical, mi_scores)}\n",
    "\n",
    "# Print results\n",
    "print(\"Mutual Information Scores:\")\n",
    "for col, score in mi_scores_rounded.items():\n",
    "    print(f\"{col}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c452a65-9aa1-4eff-a3ab-3a00de6b814d",
   "metadata": {},
   "source": [
    "**The variable with the biggest mutual information score is:**  lead_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276c62f-e528-486d-b384-514040304d1d",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc91dc86-3d97-415b-b6a7-a6606fb15457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reload and clean the data if needed\n",
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "\n",
    "# Fill missing values\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "numerical_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].fillna('NA')\n",
    "df[numerical_cols] = df[numerical_cols].fillna(0.0)\n",
    "\n",
    "# Split data\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# One-hot encode all object (categorical) columns\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_val_encoded = pd.get_dummies(X_val)\n",
    "\n",
    "# Align columns between train and val (important!)\n",
    "X_train_encoded, X_val_encoded = X_train_encoded.align(X_val_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_val_encoded)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(\"Validation Accuracy:\", round(accuracy, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baffa8cc-ceca-4bd9-80d5-bd952d75e84c",
   "metadata": {},
   "source": [
    "### Question5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1216194-f73e-498e-936f-4c1b4409c42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original accuracy: 0.7431506849315068\n",
      "Accuracy without 'industry': 0.7432 | Difference: 0.0000\n",
      "Accuracy without 'employment_status': 0.7466 | Difference: -0.0034\n",
      "Accuracy without 'lead_score': 0.7432 | Difference: 0.0000\n",
      "\n",
      " Feature with the smallest accuracy drop (least useful): employment_status\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "\n",
    "# Fill missing values\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "numerical_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "df[categorical_cols] = df[categorical_cols].fillna('NA')\n",
    "df[numerical_cols] = df[numerical_cols].fillna(0.0)\n",
    "\n",
    "# Train/val/test split\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# One-hot encode all object columns\n",
    "X_train_encoded = pd.get_dummies(X_train)\n",
    "X_val_encoded = pd.get_dummies(X_val)\n",
    "\n",
    "# Align train and val\n",
    "X_train_encoded, X_val_encoded = X_train_encoded.align(X_val_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Train original model\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_pred = model.predict(X_val_encoded)\n",
    "original_accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(\"Original accuracy:\", original_accuracy)\n",
    "\n",
    "# Now test removing features\n",
    "features_to_test = ['industry', 'employment_status', 'lead_score']\n",
    "diffs = {}\n",
    "\n",
    "for feature in features_to_test:\n",
    "    # Find all columns that came from this feature (important for one-hot encoded categoricals)\n",
    "    matching_cols = [col for col in X_train_encoded.columns if col.startswith(feature)]\n",
    "    \n",
    "    # Drop those columns\n",
    "    X_train_subset = X_train_encoded.drop(columns=matching_cols)\n",
    "    X_val_subset = X_val_encoded.drop(columns=matching_cols)\n",
    "\n",
    "    # Train and evaluate model without the feature\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    y_pred = model.predict(X_val_subset)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Calculate accuracy difference\n",
    "    diff = original_accuracy - accuracy\n",
    "    diffs[feature] = diff\n",
    "    print(f\"Accuracy without '{feature}': {accuracy:.4f} | Difference: {diff:.4f}\")\n",
    "\n",
    "# Identify feature with smallest difference\n",
    "least_useful = min(diffs, key=diffs.get)\n",
    "print(f\"\\n Feature with the smallest accuracy drop (least useful): {least_useful}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820d107a-2dc3-4e08-bb41-bb3d949585b0",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d9504ad-c884-4b0e-8d3d-937cc9f0639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 0.01: Validation Accuracy = 0.743\n",
      "C = 0.1: Validation Accuracy = 0.743\n",
      "C = 1: Validation Accuracy = 0.743\n",
      "C = 10: Validation Accuracy = 0.743\n",
      "C = 100: Validation Accuracy = 0.743\n",
      "\n",
      "✅ Best C value: 0.01 with accuracy 0.743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "results = {}\n",
    "\n",
    "\n",
    "for c in C_values:\n",
    "    model = LogisticRegression(solver='liblinear', C=c, max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "    y_pred = model.predict(X_val_encoded)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    results[c] = round(acc, 3)\n",
    "    print(f\"C = {c}: Validation Accuracy = {round(acc, 3)}\")\n",
    "\n",
    "# Finding the best C\n",
    "best_C = max(results, key=results.get)\n",
    "print(f\"\\n✅ Best C value: {best_C} with accuracy {results[best_C]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db049438-f344-431c-99db-a445e5740220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
