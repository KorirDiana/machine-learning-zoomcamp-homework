{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb707c8e-47c2-4c1a-83ae-84f3b8d54f12",
   "metadata": {},
   "source": [
    "## Homework 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d96cc-da53-4793-9797-7c9b4c0a2c0a",
   "metadata": {},
   "source": [
    "Homework\n",
    "In this homework, we'll deploy the Straight vs Curly Hair Type model we trained in the previous homework.\n",
    "\n",
    "Download the model files from here:\n",
    "\n",
    "https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx.data\n",
    "https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle/hair_classifier_v1.onnx\n",
    "With wget:\n",
    "\n",
    "PREFIX=\"https://github.com/alexeygrigorev/large-datasets/releases/download/hairstyle\"\n",
    "DATA_URL=\"${PREFIX}/hair_classifier_v1.onnx.data\"\n",
    "MODEL_URL=\"${PREFIX}/hair_classifier_v1.onnx\"\n",
    "wget ${DATA_URL}\n",
    "wget ${MODEL_URL}\n",
    "Question 1\n",
    "To be able to use this model, we need to know the name of the input and output nodes.\n",
    "\n",
    "What's the name of the output:\n",
    "\n",
    "output\n",
    "sigmoid\n",
    "softmax\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65823e71-1503-4c96-820f-2365e7d9edcd",
   "metadata": {},
   "source": [
    "### Question One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1d45e0-a3e0-4e74-a591-2429965e4f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in c:\\users\\tooter\\anaconda3\\lib\\site-packages (1.20.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from onnx) (6.33.2)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from onnx) (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d063b43-4153-4a4a-a321-c9879b024549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "input\n",
      "\n",
      "Outputs:\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"hair_classifier_v1.onnx\"\n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Check input nodes\n",
    "print(\"Inputs:\")\n",
    "for input_tensor in model.graph.input:\n",
    "    print(input_tensor.name)\n",
    "\n",
    "# Check output nodes\n",
    "print(\"\\nOutputs:\")\n",
    "for output_tensor in model.graph.output:\n",
    "    print(output_tensor.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "556a3447-5246-41e7-bcf7-ba06581af4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\tooter\\anaconda3\\lib\\site-packages (12.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eabb76c-70ba-48d2-9c0c-e98add9ef27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c531c9-47b6-4264-987d-748f5624bc4c",
   "metadata": {},
   "source": [
    "### Question Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec2f7d-f857-4ecf-9a6c-f1dd89c3e92c",
   "metadata": {},
   "source": [
    "\n",
    "Let's download and resize this image:\n",
    "\n",
    "https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
    "\n",
    "Based on the previous homework, what should be the target size for the image?\n",
    "\n",
    "64x64\n",
    "128x128\n",
    "200x200\n",
    "256x256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcc6aa-5044-4311-b859-14c759c1a493",
   "metadata": {},
   "source": [
    "**Answer = 200×200**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b16d5b-7b4a-4e55-8449-f5489690887a",
   "metadata": {},
   "source": [
    "### Question 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d190f39-0897-4db4-8505-437a05b2eb28",
   "metadata": {},
   "source": [
    "Now we need to turn the image into numpy array and pre-process it.\n",
    "\n",
    "Tip: Check the previous homework. What was the pre-processing we did there?\n",
    "\n",
    "After the pre-processing, what's the value in the first pixel, the R channel?\n",
    "\n",
    "-10.73\n",
    "-1.073\n",
    "1.073\n",
    "10.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40303341-e207-47a6-893e-1885e1434476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0732939435925546"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "\n",
    "# download helper\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    return Image.open(BytesIO(buffer))\n",
    "\n",
    "# preprocess helper (same as in training)\n",
    "def prepare_image(img, target_size=(200, 200)):\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# Imagenet normalization\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std  = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# download + resize\n",
    "url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "img = download_image(url)\n",
    "img = prepare_image(img, (200, 200))\n",
    "\n",
    "# convert to numpy (HWC → float32)\n",
    "x = np.array(img).astype(\"float32\") / 255.0\n",
    "\n",
    "# normalize\n",
    "x_norm = (x - mean) / std\n",
    "\n",
    "# first pixel, R channel\n",
    "first_pixel_R = x_norm[0, 0, 0]\n",
    "first_pixel_R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb9b65-6aa8-4d29-9c0c-4446e7789c94",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf2f3c-0a96-4c31-8e22-c5a25e57556b",
   "metadata": {},
   "source": [
    "**Now let's apply this model to this image. What's the output of the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d42e0d8e-ad85-45e3-a5e2-dfefa2c56e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[-0.20132379]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "\n",
    "# ----- helpers -----\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    return Image.open(BytesIO(buffer))\n",
    "\n",
    "def prepare_image(img, target_size=(200, 200)):\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# ----- load model -----\n",
    "session = ort.InferenceSession(\"hair_classifier_v1.onnx\")\n",
    "input_name  = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# ----- download + preprocessing -----\n",
    "url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "img = download_image(url)\n",
    "img = prepare_image(img, (200, 200))\n",
    "\n",
    "# convert to float32 array\n",
    "x = np.array(img).astype(\"float32\")\n",
    "\n",
    "# correct HW2/HW3 normalization: scale to [-1, 1]\n",
    "x = x / 255.0\n",
    "x = (x - 0.5) * 2.0\n",
    "\n",
    "# HWC → CHW\n",
    "x = np.transpose(x, (2, 0, 1))\n",
    "\n",
    "# add batch dimension\n",
    "x = np.expand_dims(x, 0)\n",
    "\n",
    "# ----- run inference -----\n",
    "output = session.run([output_name], {input_name: x})[0]\n",
    "print(\"Model output:\", output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bd13f43-9913-4ab2-b8a0-5516ec078a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[-0.20132379]]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "\n",
    "# ----- helpers -----\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    return Image.open(BytesIO(buffer))\n",
    "\n",
    "def prepare_image(img, target_size=(200, 200)):\n",
    "    if img.mode != \"RGB\":\n",
    "        img = img.convert(\"RGB\")\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# ----- load model -----\n",
    "session = ort.InferenceSession(\"hair_classifier_v1.onnx\")\n",
    "input_name  = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# ----- download + preprocessing -----\n",
    "url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "img = download_image(url)\n",
    "img = prepare_image(img, (200, 200))\n",
    "\n",
    "# convert to float32 array\n",
    "x = np.array(img).astype(\"float32\")\n",
    "\n",
    "# correct HW2/HW3 normalization: scale to [-1, 1]\n",
    "x = x / 255.0\n",
    "x = (x - 0.5) * 2.0\n",
    "\n",
    "# HWC → CHW\n",
    "x = np.transpose(x, (2, 0, 1))\n",
    "\n",
    "# add batch dimension\n",
    "x = np.expand_dims(x, 0)\n",
    "\n",
    "# ----- run inference -----\n",
    "output = session.run([output_name], {input_name: x})[0]\n",
    "print(\"Model output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472898a-9466-4122-9183-e67745815b8b",
   "metadata": {},
   "source": [
    "Prepare the lambda code\n",
    "Now you need to copy all the code into a separate python file. You will need to use this file for the next two questions.\n",
    "\n",
    "Tip: you can test this file locally with ipython or Jupyter Notebook by importing the file and invoking the function from this file.\n",
    "\n",
    "Docker\n",
    "For the next two questions, we'll use a Docker image that we already prepared. This is the Dockerfile that we used for creating the image:\n",
    "\n",
    "FROM public.ecr.aws/lambda/python:3.13\n",
    "\n",
    "COPY hair_classifier_empty.onnx.data .\n",
    "COPY hair_classifier_empty.onnx .\n",
    "Note that it uses Python 3.13.\n",
    "\n",
    "The docker image is published to agrigorev/model-2025-hairstyle:v1.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "The image already contains a model and it's not the same model as the one we used for questions 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a1fe0-73f9-4ab2-a20c-5128cd376c22",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df878e48-642d-406a-ad4b-e42ce42878f3",
   "metadata": {},
   "source": [
    "Download the base image agrigorev/model-2025-hairstyle:v1. You can do it with docker pull.\n",
    "\n",
    "So what's the size of this base image?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b151c5c-9b00-4f92-8699-12b40c8f1c03",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13031a39-ba57-49d8-9c98-8497d45c5d5a",
   "metadata": {},
   "source": [
    "Now let's extend this docker image, install all the required libraries and add the code for lambda.\n",
    "\n",
    "You don't need to include the model in the image. It's already included. The name of the file with the model is hair_classifier_empty.onnx and it's in the current workdir in the image (see the Dockerfile above for the reference). The provided model requires the same preprocessing for images regarding target size and rescaling the value range than used in homework 8.\n",
    "\n",
    "Now run the container locally.\n",
    "\n",
    "Score this image: https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\n",
    "\n",
    "What's the output from the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c0dcc-19f1-4065-baa1-f628f64622b3",
   "metadata": {},
   "source": [
    "**-0.10**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb7a4f-c90d-4383-bc46-756df340cd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
