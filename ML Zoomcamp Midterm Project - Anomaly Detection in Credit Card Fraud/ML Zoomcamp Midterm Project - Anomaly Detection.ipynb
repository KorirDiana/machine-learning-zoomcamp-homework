{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7491e3f5-089a-4d3d-b5d9-3ef1dc6ad2fa",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Credit Card Fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fced6b-3d2f-4a42-a7f0-cdfd53e603ed",
   "metadata": {},
   "source": [
    "Source: https://www.analyticsvidhya.com/blog/2023/05/anomaly-detection-in-credit-card-fraud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b29378e-f8d3-4d34-97ce-28021d3aeccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0a2dd-326d-485d-b728-faa9b5a0e25b",
   "metadata": {},
   "source": [
    "**Handling Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5af17c9-0a4c-43b6-a35c-9a7f9c8bdcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any missing values in the dataset\n",
    "\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a915a09-8abe-4108-adcd-fcb8f385c046",
   "metadata": {},
   "source": [
    "**Scaling the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a035e-2912-4914-a825-e19dd157e8fa",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be sensitive to the scale of the data. Therefore, it is important to scale the data before applying the algorithm. We can use the StandardScaler class from the sklearn.preprocessing module to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c586527-d9f3-45a3-9f13-ed92e0c0fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0 -1.996583 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
      "1 -1.996583  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
      "2 -1.996562 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
      "3 -1.996562 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
      "4 -1.996541 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
      "\n",
      "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
      "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28    Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  0.244964      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724 -0.342475      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  1.160686      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  0.140534      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153 -0.073403      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the Amount column\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Scale the Time column\n",
    "df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "# Check the first few rows of the dataset after scaling\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05232f7-7764-4652-b6d0-cb20c40fd058",
   "metadata": {},
   "source": [
    "## **Isolation Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e2bf9-858d-4baf-ace2-d973c780ce67",
   "metadata": {},
   "source": [
    "Isolation Forest is a popular algorithm for anomaly detection that is based on the concept of decision trees. It works by creating random decision trees for the given data and isolating the anomalies by creating shorter paths for them.\n",
    "\n",
    "Let’s implement the Isolation Forest algorithm on our credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9d4e476-ebdb-4dbb-aeeb-8cbd4fa10078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 2849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create the Isolation Forest object\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.01),\n",
    " max_features=1.0, random_state=42)\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "clf.fit(df)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = clf.predict(df)\n",
    "\n",
    "# Reshape the predictions to a 1D array\n",
    "y_pred = y_pred.reshape(-1,1)\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"Number of outliers:\", len(df[y_pred == -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f149f2-feb4-4f1a-b56e-8cc0c654d991",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has detected 2848 anomalies in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b91f45-1dbd-466b-b400-96da91d69560",
   "metadata": {},
   "source": [
    "## **Local Outlier Factor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70839dde-6de4-4504-b179-188c7c4a13d7",
   "metadata": {},
   "source": [
    "Local Outlier Factor (LOF) is another popular algorithm for anomaly detection that is based on the concept of local density. It works by calculating the density of a data point relative to its neighbors and identifying points that have a much lower density than their neighbors as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e16922-4215-41f7-9ef7-b136ca9ab154",
   "metadata": {},
   "source": [
    "Implementing the LOF algorithm on the credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9d7705-2616-439b-9d33-dbca5b32ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 2849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Create the LOF object\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=float(0.01))\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "y_pred = clf.fit_predict(df)\n",
    "\n",
    "# Reshape the predictions to a 1D array\n",
    "y_pred = y_pred.reshape(-1,1)\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"Number of outliers:\", len(df[y_pred == -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d6d33-7cb9-46fb-8bb2-5ea227c7f8d5",
   "metadata": {},
   "source": [
    "The LOF algorithm has also detected 2848 anomalies in the dataset, which is the same as the Isolation Forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc4e04-c1ae-427f-885a-699438504ffc",
   "metadata": {},
   "source": [
    "## **One-class SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c05030-826c-4135-9803-2ec16b776d11",
   "metadata": {},
   "source": [
    "One-class SVM is another popular algorithm for anomaly detection that is based on the concept of maximum margin hyperplanes. It works by creating a hyperplane that separates the normal data points from the anomalies and identifying points that lie on the wrong side of the hyperplane as anomalies.\n",
    "\n",
    "Implementing the One-class SVM algorithm on the credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f27e1aa-c353-4b62-a409-0f8bd52d7f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m OneClassSVM(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fit the data and tag the outliers\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(df)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the predictions\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:1768\u001b[0m, in \u001b[0;36mOneClassSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Detect the soft boundary of the set of samples X.\u001b[39;00m\n\u001b[0;32m   1745\u001b[0m \n\u001b[0;32m   1746\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;124;03m    If X is not a C-ordered contiguous array it is copied.\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, np\u001b[38;5;241m.\u001b[39mones(_num_samples(X)), sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_\n\u001b[0;32m   1770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m fit(X, y, sample_weight, solver_type, kernel, random_seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[0;32m    319\u001b[0m (\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m libsvm\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[0;32m    332\u001b[0m     svm_type\u001b[38;5;241m=\u001b[39msolver_type,\n\u001b[0;32m    333\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    334\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_weight_\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    335\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[0;32m    336\u001b[0m     C\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m    337\u001b[0m     nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnu,\n\u001b[0;32m    338\u001b[0m     probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability,\n\u001b[0;32m    339\u001b[0m     degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[0;32m    340\u001b[0m     shrinking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshrinking,\n\u001b[0;32m    341\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    342\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[0;32m    343\u001b[0m     coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[0;32m    344\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[0;32m    345\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon,\n\u001b[0;32m    346\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m    347\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mrandom_seed,\n\u001b[0;32m    348\u001b[0m )\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "File \u001b[1;32msklearn\\\\svm\\\\_libsvm.pyx:265\u001b[0m, in \u001b[0;36msklearn.svm._libsvm.fit\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Create the One-class SVM object\n",
    "clf = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.01)\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "clf.fit(df)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = clf.predict(df)\n",
    "\n",
    "# Reshape the predictions to a 1D array\n",
    "y_pred = y_pred.reshape(-1,1)\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"Number of outliers:\", len(df[y_pred == -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bcddd-a223-48ae-947e-8e3f4acbd055",
   "metadata": {},
   "source": [
    "The One-class SVM algorithm has detected 492 anomalies in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84eaeab2-a2e6-4be2-9755-28842a2af6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f18a5157-38e2-4636-8bdd-4899c7b45735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c19db07a-9a21-4b7f-9198-e1f813e9484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.99916234        nan 0.99915732        nan 0.99916735]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.87      0.64      0.74       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.93      0.82      0.87     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "DecisionTreeClassifier\n",
      "{'criterion': 'entropy', 'max_depth': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.91      0.79      0.85       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.90      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a list of classifiers to evaluate\n",
    "classifiers = [LogisticRegression(), DecisionTreeClassifier()]\n",
    "\n",
    "# Create parameter grids for each classifier\n",
    "lr_params = {'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10]}\n",
    "dt_params = {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, 7]}\n",
    "rf_params = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7]}\n",
    "knn_params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "param_grids = [lr_params, dt_params, rf_params, knn_params]\n",
    "\n",
    "# Loop over classifiers and parameter grids to find the best model\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    clf = GridSearchCV(classifier, param_grids[i], cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classifier.__class__.__name__)\n",
    "    print(clf.best_params_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fae70f-dd7b-402d-8f92-ed6f9b43f229",
   "metadata": {},
   "source": [
    "In this code, we have evaluated the performance of our models using cross-validation and selected the best performing model. We have used the stratified K-fold cross-validation technique to split the dataset into 5 folds, ensuring that the proportion of fraud cases is the same in each fold. Then, we have trained and evaluated three models – Logistic Regression, Decision Tree – using the cross-validation technique. We have used the average precision score as the evaluation metric because it is a suitable metric for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f067c1fc-baf9-4c51-852e-ba37347158a7",
   "metadata": {},
   "source": [
    "## **Model Deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba5c11-3a3d-4db0-b453-da8682d188c7",
   "metadata": {},
   "source": [
    "The final step in the machine learning pipeline is to deploy the selected model to make predictions on new data. In this step, we will use the selected model to make predictions on the test dataset and evaluate its performance using classification metrics.\n",
    "\n",
    "We will use the predict method of the trained model to make predictions on the test data, and then evaluate the model’s performance using the accuracy_score, precision_score, recall_score, and f1_score metrics from the sklearn.metrics module.\n",
    "\n",
    "The code for this step is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412ec22-e735-40d8-88a0-d0d18497cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print the classification metrics\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139330c-f421-4759-bce6-09646aafdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this code, we first use the predict method of the trained rf_model to make predictions on the test set X_test. We then evaluate the model’s performance using the accuracy_score, precision_score, recall_score, and f1_score metrics. Finally, we print the classification metrics to the console.\n",
    "\n",
    "Note that we have imported the required metrics from the sklearn.metrics module. These metrics help us to evaluate the performance of the model and make\n",
    "informed decisions about its suitability for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fa9bc-5bb4-4e05-83c6-f01ffed2eb08",
   "metadata": {},
   "source": [
    "## **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97628d-38a5-423c-8a81-7057c42e876e",
   "metadata": {},
   "source": [
    "In this article, we have discussed the concept of anomaly detection and various algorithms that can be used to detect anomalies in a dataset. We have also implemented some of these algorithms in Python and applied them to a credit card fraud dataset to detect anomalies. It is important to note that the choice of algorithm and the preprocessing techniques depend on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13df45-dcfd-476d-ac72-6b5edb2794b5",
   "metadata": {},
   "source": [
    "**Key Takeaways for Anomaly Detection in credit card fraud**\n",
    "\n",
    "- Anomaly detection is used to detect unusual data points or patterns in a dataset and can be applied in various fields such as finance, healthcare, and cybersecurity.\n",
    "- The choice of algorithm and preprocessing techniques should be based on the nature of the data and the problem at hand.\n",
    "- The isolation forest algorithm is based on random forests. It is effective in detecting point anomalies and can be a suitable option for anomaly detection in some cases.\n",
    "- Preprocessing techniques such as scaling and feature selection can improve the accuracy of the model. It should be considered when implementing anomaly detection.\n",
    "- As the amount of data continues to grow, stay up-to-date with the latest algorithms and techniques to improve the accuracy and effectiveness of anomaly detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01f824-75d9-4dfe-bbc9-a40fd26adfed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
