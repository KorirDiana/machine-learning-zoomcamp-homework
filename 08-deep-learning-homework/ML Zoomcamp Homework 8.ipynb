{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d542b572-a437-4be9-8a78-f8fe626d3aa2",
   "metadata": {},
   "source": [
    "# ML Zoomcamp Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdab22-daa4-4a58-838a-7802212e6c9e",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fa1d39-11a1-41ee-85ca-f742b6b3c63f",
   "metadata": {},
   "source": [
    "**Which loss function you will use?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2fd3f-6290-4b04-8f41-93664eaaf6d5",
   "metadata": {},
   "source": [
    "nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2137d-c28a-430b-9b6e-1ab20e06a0f0",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f31e6-86eb-4592-9896-72a1fab1156f",
   "metadata": {},
   "source": [
    "What's the total number of parameters of the model? You can use torchsummary or count manually.\n",
    "\n",
    "In PyTorch, you can find the total number of parameters using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a6d0668-4e34-4e6e-b9f1-2c0f21b0bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\tooter\\anaconda3\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f23bc423-3989-4f5e-ab38-78de0b8a6e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\tooter\\anaconda3\\lib\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da6a8653-919c-475b-b4a8-fbb2e725bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HairCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(HairCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 99 * 99, 64)  # 200x200 -> 198 -> pooled -> 99\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = HairCNN(num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b12b4775-7660-4f91-a228-419bacad1dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HairCNN                                  [1, 5]                    --\n",
       "├─Conv2d: 1-1                            [1, 32, 198, 198]         896\n",
       "├─MaxPool2d: 1-2                         [1, 32, 99, 99]           --\n",
       "├─Linear: 1-3                            [1, 64]                   20,072,512\n",
       "├─Linear: 1-4                            [1, 5]                    325\n",
       "==========================================================================================\n",
       "Total params: 20,073,733\n",
       "Trainable params: 20,073,733\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 55.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.48\n",
       "Forward/backward pass size (MB): 10.04\n",
       "Params size (MB): 80.29\n",
       "Estimated Total Size (MB): 90.81\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, 3, 200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6b00b44-8dea-4994-9146-13a70884c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 20073733\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Manual counting\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad9dd9ee-348b-449b-b88e-82b726eac979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cpu'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "04ec4477-9270-40e8-8e6e-bd02a73f16ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchvision 0.17.2+cpu\n",
      "Uninstalling torchvision-0.17.2+cpu:\n",
      "  Successfully uninstalled torchvision-0.17.2+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Tooter\\anaconda3\\Lib\\site-packages\\~orchvision'.\n",
      "You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6c4c689b-ff34-4699-aa69-439f96d8f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\n",
      "ERROR: Could not find a version that satisfies the requirement torchvision==0.15.1 (from versions: 0.17.0, 0.17.1, 0.17.2, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.20.0, 0.20.1, 0.21.0, 0.22.0, 0.22.1, 0.23.0, 0.24.0, 0.24.1)\n",
      "ERROR: No matching distribution found for torchvision==0.15.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c50f005-f089-4f49-b0da-2eb0176725ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.2.2+cpu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchvision as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uninstalling torch-2.2.2+cpu:\n",
      "  Successfully uninstalled torch-2.2.2+cpu\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.7.0+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.7.0%2Bcpu-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision==0.17.0+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.17.0%2Bcpu-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.7.0+cpu) (69.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torchvision==0.17.0+cpu) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torchvision==0.17.0+cpu) (2.32.4)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "\n",
      "The conflict is caused by:\n",
      "    The user requested torch==2.7.0+cpu\n",
      "    torchvision 0.17.0+cpu depends on torch==2.2.0\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cannot install torch==2.7.0+cpu and torchvision==0.17.0+cpu because these package versions have conflicting dependencies.\n",
      "ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torchvision torch\n",
    "!pip install torch==2.7.0+cpu torchvision==0.17.0+cpu --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a351f5cc-75bc-44b0-a19e-9e7e5697f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torch==2.9.1+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.9.1+cpu) (69.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.9.1+cpu) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from jinja2->torch==2.9.1+cpu) (2.1.3)\n",
      "Using cached https://download.pytorch.org/whl/cpu/torch-2.9.1%2Bcpu-cp312-cp312-win_amd64.whl (110.9 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.9.1+cpu --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b227ae13-eb67-4851-b937-996581ba2c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Collecting torchvision==0.17.2+cpu\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.17.2%2Bcpu-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torchvision==0.17.2+cpu) (1.26.4)\n",
      "Collecting torch==2.2.2 (from torchvision==0.17.2+cpu)\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-2.2.2%2Bcpu-cp312-cp312-win_amd64.whl (200.7 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torchvision==0.17.2+cpu) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (4.14.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from torch==2.2.2->torchvision==0.17.2+cpu) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from jinja2->torch==2.2.2->torchvision==0.17.2+cpu) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tooter\\anaconda3\\lib\\site-packages (from sympy->torch==2.2.2->torchvision==0.17.2+cpu) (1.3.0)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.9.1+cpu\n",
      "    Uninstalling torch-2.9.1+cpu:\n",
      "      Successfully uninstalled torch-2.9.1+cpu\n",
      "Successfully installed torch-2.2.2+cpu torchvision-0.17.2+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision==0.17.2+cpu --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73f3f1ce-e845-43c1-b7fb-8e41ba0c112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Transform (resize, tensor, normalize)\n",
    "class Transform:\n",
    "    def __init__(self, resize=(200,200), mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)):\n",
    "        self.resize = resize\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.resize)\n",
    "        img = torch.tensor(list(img.getdata()), dtype=torch.float32).view(img.size[1], img.size[0], 3).permute(2,0,1)/255.0\n",
    "        for c in range(3):\n",
    "            img[c] = (img[c] - self.mean[c]) / self.std[c]\n",
    "        return img\n",
    "\n",
    "# Custom dataset\n",
    "class HairDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for label, class_name in enumerate(os.listdir(root_dir)):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                self.samples.append((os.path.join(class_dir, fname), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a4ca061-c035-42a3-a030-2eb2762d58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HairDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        # Each subfolder is a class\n",
    "        self.classes = sorted(os.listdir(root_dir))  # ['curled', 'dreadlocks', 'kinky', 'straight', 'wavy']\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        for cls_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, cls_name)\n",
    "            for fname in os.listdir(class_dir):\n",
    "                self.samples.append((os.path.join(class_dir, fname), self.class_to_idx[cls_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e747fdb4-81bc-4a03-bb29-afe1396761f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HairCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):  # 5 classes\n",
    "        super(HairCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32*99*99, 64)  # after conv + pool: (200-3+1)/2 = 99\n",
    "        self.fc2 = nn.Linear(64, num_classes)  # output neurons = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # no sigmoid, use CrossEntropyLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "913de93a-ed1d-45bd-9d13-c91557b0a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65e44c1-a467-4b40-94d9-810dc9dfcbd2",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a0827-44f5-49ba-8428-70cc7ea9694d",
   "metadata": {},
   "source": [
    "**What is the median of training accuracy for all the epochs for this model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b6a814c1-1fed-4ab1-94b4-fff34610aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # number of times to iterate over the full training dataset\n",
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73c60d17-5d77-4907-b91e-2fe8682109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define paths\n",
    "data_dir = r\"C:\\Users\\Tooter\\data\"\n",
    "\n",
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create dataset using ImageFolder (automatically assigns class labels based on subfolder names)\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=train_transforms)\n",
    "\n",
    "# Split dataset into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b06881a8-819f-4737-9af0-e5608ab77f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "median_acc = np.median(history['acc'])\n",
    "print(f\"Median training accuracy: {median_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f8101eb-cbff-49c6-8f56-9144e8018e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "data_dir = r\"C:\\Users\\Tooter\\data\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "978ed868-a357-4b8f-a9dc-c3314bba2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class HairCNN(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(HairCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(32*99*99, 64)  # compute size after conv+pool\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = HairCNN(num_classes=5).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002, momentum=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "646e8ef1-4558-4822-a2d8-cc00a2a37d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop finished.\n"
     ]
    }
   ],
   "source": [
    "history = {'acc': [], 'loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    history['loss'].append(epoch_loss)\n",
    "    history['acc'].append(epoch_acc)\n",
    "\n",
    "print(\"Training loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fd3c3cd-4e13-454c-9201-d2d096a6da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "data_dir = r\"C:\\Users\\Tooter\\data\"\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((200, 200)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Split into train and validation (80/20 split)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, validation_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db7dbbe2-e10b-42cf-b707-ec9d7f8825b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median training accuracy: 0.6979\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "median_acc = np.median(history['acc'])\n",
    "print(f\"Median training accuracy: {median_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ea6be2-c96d-4db3-9ef1-240f614868f4",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c213e8c2-c30a-46f1-b057-133e86b4edc8",
   "metadata": {},
   "source": [
    "**What is the standard deviation of training loss for all the epochs for this model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6279d965-9b84-4f92-b7fe-bd4830c1b7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of training loss: 0.045\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: history dictionary from training\n",
    "# Replace these with your actual epoch losses\n",
    "history = {\n",
    "    'loss': [0.693, 0.612, 0.587, 0.574, 0.563, 0.555, 0.550, 0.545, 0.540, 0.536]\n",
    "}\n",
    "\n",
    "# Compute standard deviation\n",
    "std_loss = np.std(history['loss'])\n",
    "print(f\"Standard deviation of training loss: {std_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97a3ab-b4de-4580-b984-985076befe69",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a67a9-7012-471c-9d23-860d86da6fda",
   "metadata": {},
   "source": [
    "**Let's train our model for 10 more epochs using the same code as previously. Note: make sure you don't re-create the model. \n",
    "we want to continue training the model we already started training. \n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0fcbfa0f-c99c-4837-9cc5-ba91392f3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 test loss: 0.3988\n",
      "Epoch 2 test loss: 0.3988\n",
      "Epoch 3 test loss: 0.3988\n",
      "Epoch 4 test loss: 0.3988\n",
      "Epoch 5 test loss: 0.3988\n",
      "Epoch 6 test loss: 0.3988\n",
      "Epoch 7 test loss: 0.3988\n",
      "Epoch 8 test loss: 0.3988\n",
      "Epoch 9 test loss: 0.3988\n",
      "Epoch 10 test loss: 0.3988\n",
      "\n",
      "Mean test loss over 10 epochs: 0.3988\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Make sure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store losses for each epoch\n",
    "test_losses = []\n",
    "\n",
    "num_epochs = 10  # continuing training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:  # or test_loader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # If using BCEWithLogitsLoss, ensure labels are float and shaped (batch_size,1)\n",
    "            # labels = labels.float().unsqueeze(1)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    test_losses.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch+1} test loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Compute mean test loss across all epochs\n",
    "mean_test_loss = np.mean(test_losses)\n",
    "print(f\"\\nMean test loss over {num_epochs} epochs: {mean_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be5868-3faf-4f17-9089-7e41c2335955",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1fac4-fefc-4377-a4fb-f51f41626352",
   "metadata": {},
   "source": [
    "**What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "96cf6537-e10b-42d3-9393-5f3969600825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 test accuracy: 0.8997\n",
      "Epoch 2 test accuracy: 0.8997\n",
      "Epoch 3 test accuracy: 0.8997\n",
      "Epoch 4 test accuracy: 0.8997\n",
      "Epoch 5 test accuracy: 0.8997\n",
      "Epoch 6 test accuracy: 0.8997\n",
      "Epoch 7 test accuracy: 0.8997\n",
      "Epoch 8 test accuracy: 0.8997\n",
      "Epoch 9 test accuracy: 0.8997\n",
      "Epoch 10 test accuracy: 0.8997\n",
      "\n",
      "Average test accuracy for last 5 epochs: 0.8997\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assume model is already trained or being continued for more epochs\n",
    "model.eval()\n",
    "\n",
    "# Store test/validation accuracy per epoch\n",
    "val_acc_list = []\n",
    "\n",
    "num_epochs = 10  # continuing training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validation_loader:  # or test_loader\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # For BCEWithLogitsLoss (binary classification)\n",
    "            # outputs = model(images)\n",
    "            # predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "            # For multi-class classification with CrossEntropyLoss\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_acc = correct / total\n",
    "    val_acc_list.append(epoch_acc)\n",
    "    print(f\"Epoch {epoch+1} test accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "# Compute average accuracy for last 5 epochs (6–10)\n",
    "avg_last5_acc = np.mean(val_acc_list[-5:])\n",
    "print(f\"\\nAverage test accuracy for last 5 epochs: {avg_last5_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ccac4-3b44-4c1d-976d-03798c8fe765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
